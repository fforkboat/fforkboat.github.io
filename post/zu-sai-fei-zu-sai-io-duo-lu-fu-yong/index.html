<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes" />
<title>阻塞、非阻塞、IO多路复用 | fforkboat&#39;s dock</title>
<link rel="shortcut icon" href="https://fforkboat.github.io/favicon.ico">
<link href="https://fforkboat.github.io/styles/main.css" rel="stylesheet">
<link href="//at.alicdn.com/t/font_1678829_ntebi130zaa.css" rel="stylesheet">
<link rel="alternate" type="application/rss+xml" title="fforkboat&#39;s dock » Feed" href="https://fforkboat.github.io/atom.xml">

  <meta name="description" content="
文章转自：Blocking I/O, Nonblocking I/O, And Epoll

In this post I want to explain exactly what happens when you use nonbloc..." />
  <meta name="keywords" content="" />

  <link href="https://cdn.bootcdn.net/ajax/libs/twitter-bootstrap/2.3.1/css/bootstrap.css" rel="stylesheet">
  <link href="https://fforkboat.github.io/media/css/jquery.tocify.css" rel="stylesheet">

  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
  <script src="https://fforkboat.github.io/media/scripts/jquery.ui.core.js"></script>
  <script src="https://fforkboat.github.io/media/scripts/bootstrap.js"></script>
  <script src="https://fforkboat.github.io/media/scripts/jquery.tocify.js"></script>

  <style>
    #toc .nav-list > .active > a{
      color: #007fff;
      text-shadow: 0 -1px 0 rgba(0, 0, 0, 0.2);
      background-color: #ebedef;
    }
    #toc a {
      color: #000000;
    }
    #toc {
      border: 0;
    }
  </style>
</head>

<body style="height: auto">
  <div id="toc" style="right: 20px; top: 80px"></div>
  <div class="main animated">
    <div class="header animated fadeInDown">
  <div class="site_title_container">
    <div class="site_title">
      <a href="https://fforkboat.github.io">
        fforkboat&#39;s dock
      </a>
    </div>
  </div>
  <div class="my_socials">
    
    
          
    
          
    
          
    
          
    
          
    <a href="https://fforkboat.github.io/atom.xml" target="_blank">
      <i class="iconfont icon-rss"></i></a>
  </div>
</div>

<div class="header_menu">
  
  
  <a href="/" class="menu">首页</a>
  

  
  <a href="/archives" class="menu">归档</a>
  

  
  <a href="/tags" class="menu">标签</a>
  

  
  <a href="/post/about" class="menu">关于</a>
  

  <div class="gridea-search-div">
    <form id="gridea-search-form" action="https://fforkboat.github.io/search/">
      <input class="gridea-search-input" autocomplete="off" spellcheck="false" name="q" />
    </form>
  </div>
</div>

    <div class="autopagerize_page_element">
      <div class="content">
        <div class="post_page">
          <div class="post animated fadeInDown">
            <div class="post_title post_detail_title">
              <h1 class="indicator-free">
                <a>
                  阻塞、非阻塞、IO多路复用</a>
              </h2>
              <span class="article-info">2020-09-06, 1916 words, 12 min read</span>
            </div>
            <div class="post_content markdown">
              <p class="md_block">
                <span class="md_line md_line_start md_line_end">
                  <blockquote>
<p>文章转自：<a href="https://eklitzke.org/blocking-io-nonblocking-io-and-epoll">Blocking I/O, Nonblocking I/O, And Epoll</a></p>
</blockquote>
<p>In this post I want to explain exactly what happens when you use nonblocking I/O. In particular, I want to explain:</p>
<ul>
<li>The semantics of setting <code>O_NONBLOCK</code> on a file descriptor using <code>fcntl</code></li>
<li>How nonblocking I/O is different from asynchronous I/O</li>
<li>Why nonblocking I/O is frequently used in conjunction with I/O multiplexers like <code>select</code>, <code>epoll</code>, and <code>kqueue</code></li>
<li>How nonblocking mode interacts with edge-triggered polling with <code>epoll</code></li>
</ul>
<h2 id="blocking-mode">Blocking Mode</h2>
<p>By default, all file descriptors on Unix systems start out in “blocking mode”. That means that I/O system calls like <code>read</code>, <code>write</code>, or <code>connect</code> can block. A really easy way to understand this is to think about what happens when you read data on stdin from a regular TTY-based program. If you call <code>read</code> on stdin then your program will block until data is actually available, such as when the user actually physically types characters on their keyboard. Specifically, the kernel will put the process into the “sleeping” state until data is available on stdin. This is also the case for other types of file descriptors. For instance, if you try to read from a TCP socket then the <code>read</code> call will block until the other side of the connection actually sends data.</p>
<p>Blocking is a problem for programs that should operate concurrently, since blocked processes are suspended. There are two different, complementary ways to solve this problem. They are:</p>
<ul>
<li>Nonblocking mode</li>
<li>I/O multiplexing system calls, such as <code>select</code> and <code>epoll</code></li>
</ul>
<p>These two solutions are often used together, but they are independent strategies to solving this problem, and often both are used. In a moment we’ll see the difference and why they’re commonly both used.</p>
<h2 id="nonblocking-mode-o_nonblock">Nonblocking Mode (O_NONBLOCK)</h2>
<p>A file descriptor is put into “nonblocking mode” by adding <code>O_NONBLOCK</code> to the set of <code>fcntl</code> flags on the file descriptor:</p>
<pre><code class="language-c">/* set O_NONBLOCK on fd */
int flags = fcntl(fd, F_GETFL, 0);
fcntl(fd, F_SETFL, flags | O_NONBLOCK);
</code></pre>
<p>From this point forward the file descriptor is considered nonblocking. When this happens I/O system calls like <code>read</code> and <code>write</code> that would block will return -1, and <code>errno</code> will be set to <code>EWOULDBLOCK</code>.</p>
<p>This is interesting, but on its own is actually not that useful. With just this primitive there’s no efficient way to do I/O on multiple file descriptors. For instance, suppose we have two file descriptors and want to read both of them at once. This could be accomplished by having a loop that checks each file descriptor for data, and then sleeps momentarily before checking again:</p>
<pre><code class="language-c">struct timespec sleep_interval{.tv_sec = 0, .tv_nsec = 1000};
ssize_t nbytes;
for (;;) {
    /* try fd1 */
    if ((nbytes = read(fd1, buf, sizeof(buf))) &lt; 0) {
        if (errno != EWOULDBLOCK) {
            perror(&quot;read/fd1&quot;);
        }
    } else {
        handle_data(buf, nbytes);
    }

    /* try fd2 */
    if ((nbytes = read(fd2, buf, sizeof(buf))) &lt; 0) {
        if (errno != EWOULDBLOCK) {
            perror(&quot;read/fd2&quot;);
        }
    } else {
        handle_data(buf, nbytes);
    }

    /* sleep for a bit; real version needs error checking! */
    nanosleep(sleep_interval, NULL);
}
</code></pre>
<p>This works, but has a lot of drawbacks:</p>
<ul>
<li>When data is coming in very slowly the program will wake up frequently and unnecessarily, which wastes CPU resources.</li>
<li>When data does come in, the program may not read it immediately if it’s sleeping, so the latency of the program will be poor.</li>
<li>Handling a large number of file descriptors with this pattern would become cumbersome.</li>
</ul>
<p>To fix these problems we need an I/O multiplexer.</p>
<h2 id="io-multiplexing-select-epoll-kqueue-etc">I/O Multiplexing (select, epoll, kqueue, etc.)</h2>
<p>There’s a few I/O multiplexing system calls. Examples of I/O multiplexing calls include <code>select</code> (defined by POSIX), the <a href="http://man7.org/linux/man-pages/man7/epoll.7.html">epoll family</a> on Linux, and the <a href="https://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;sektion=2">kqueue family</a> on BSD. These all work fundamentally the same way: they let the kernel know what events (typically read events and write events) are of interest on a set of file descriptors, and then they block until something of interest happens. For instance, you might tell the kernel you are interested in just read events on file descriptor <em>X</em>, both read and write events on file descriptor <em>Y</em>, and just write events on file descriptor <em>Z</em>.</p>
<p>These I/O multiplexing system calls typically do not care if the file descriptors are in blocking mode or nonblocking mode. You can leave all of your file descriptors in blocking mode and they’ll work just fine with <code>select</code> or <code>epoll</code>. If you only call <code>read</code> and <code>write</code> on file descriptors returned by <code>select</code> or <code>epoll</code> the calls won’t block, even if those file descriptors are in blocking mode. There’s one important exception! The blocking or nonblocking status of a file descriptor is significant for edge-triggered polling, as explained further below.</p>
<p>The multiplexing approach to concurrency is what I call “asynchronous I/O”. Sometimes people will call this same approach “nonblocking I/O”, which I believe comes from a confusion about what “nonblocking” means at the systems programming level. I suggest reserving the term “nonblocking” for referring to whether or not file descriptors are actually in nonblocking mode or not.</p>
<h2 id="how-o_nonblock-interacts-with-io-multiplexing">How O_NONBLOCK Interacts With I/O Multiplexing</h2>
<p>Let’s say we’re writing a simple socket server using <code>select</code> with blocking file descriptors. For simplicity, in this example we just have file descriptors we want to read from, which are in <code>read_fds</code>. The core part of the event loop will call <code>select</code> and then invoke <code>read</code> once for each file descriptor with data:</p>
<pre><code class="language-c">ssize_t nbytes;
for (;;) {
    /* select call happens here */
    if (select(FD_SETSIZE, &amp;read_fds, NULL, NULL, NULL) &lt; 0) {
        perror(&quot;select&quot;);
        exit(EXIT_FAILURE);
    }
    for (int i = 0; i &lt; FD_SETSIZE; i++) {
        if (FD_ISSET(i, &amp;read_fds)) {
            /* read call happens here */
            if ((nbytes = read(i, buf, sizeof(buf))) &gt;= 0) {
                handle_read(nbytes, buf);
            } else {
                /* real version needs to handle EINTR correctly */
                perror(&quot;read&quot;);
                exit(EXIT_FAILURE);
            }
        }
    }
}
</code></pre>
<p>This works and it’s perfectly fine. But, what happens if <code>buf</code> is small, and a lot of data comes down the line? To be concrete, suppose that <code>buf</code> is a 1024-byte buffer but 64KB of data comes in all at once. To handle this request we’ll invoke <code>select</code> followed by <code>read</code> 64 times. That’s 128 total system calls, which is a lot.</p>
<p>If the buffer size is too small <code>read</code> will have to be called a lot of times, there’s no avoiding that. But perhaps we could reduce the number of times we call <code>select</code>? Ideally in this example we would call <code>select</code> only one time.</p>
<p>In fact, this is possible, and it’s accomplished by putting the file descriptors into nonblocking mode. The basic idea is that you keep calling <code>read</code> in a loop until it returns <code>EWOULDBLOCK</code>. That looks like this:</p>
<pre><code class="language-c">ssize_t nbytes;
for (;;) {
    /* select call happens here */
    if (select(FD_SETSIZE, &amp;read_fds, NULL, NULL, NULL) &lt; 0) {
        perror(&quot;select&quot;);
        exit(EXIT_FAILURE);
    }
    for (int i = 0; i &lt; FD_SETSIZE; i++) {
        if (FD_ISSET(i, &amp;read_fds)) {
            /* NEW: loop until EWOULDBLOCK is encountered */
            for (;;) {
                /* read call happens here */
                nbytes = read(i, buf, sizeof(buf));
                if (nbytes &gt;= 0) {
                    handle_read(nbytes, buf);
                } else {
                    if (errno != EWOULDBLOCK) {
                        /* real version needs to handle EINTR correctly */
                        perror(&quot;read&quot;);
                        exit(EXIT_FAILURE);
                    }
                    break;
                }
            }
        }
    }
}
</code></pre>
<p>In this example (1024-byte buffer with 64KB of data incoming) we’ll do 66 system calls: <code>select</code> will be called one time, <code>read</code> will be called without error 64 times, and <code>read</code> will be called and return <code>EWOULDBLOCK</code> one time. This is much better! This is nearly half the number from the previous example, which will improve performance and scalability considerably.</p>
<p>The downside of this approach is that due to the new loop, there’s at least one extra <code>read</code> that happens since it’s called until it returns <code>EWOULDBLOCK</code>. Let’s say that typically the read buffer is large enough to read all of the incoming data in one <code>read</code> call. Then in the usual case through the loop there will be three system calls rather than just two: <code>select</code> to wait for the data, <code>read</code> to actually read the data, and then <code>read</code> again to get <code>EWOULDBLOCK</code>.</p>
<h2 id="edge-triggered-polling">Edge-Triggered Polling</h2>
<p>There’s one more important use of nonblocking I/O: with edge-triggered polling in the <code>epoll</code> system call. This system call has two modes: level-triggered polling, and edge-triggered polling. Level-triggered polling is a simpler programming model that’s similar to the classic <code>select</code> system call. To explain the difference we need to understand how <code>epoll</code> works within the kernel.</p>
<p>Suppose you tell the kernel you’re interested in using <code>epoll</code> to monitor read events on some file descriptor. The kernel maintains a list of these interests for each file descriptor. When data comes in on the file descriptor the kernel traverses the interests list and wakes up each process that was blocked in <code>epoll_wait</code> with that file descriptor in the event list.</p>
<p>What I outlined above happens regardless of what triggering mode <code>epoll</code> is in. The difference between level-triggered and edge-triggered polling is what happens in the kernel when you call <code>epoll_wait</code>. In level-triggered mode the kernel will traverse each file descriptor in the interest list to see if it already matches the interest condition. For instance, if you registered a read event on file descriptor 8, when calling <code>epoll_wait</code> the kernel will first check: does file descriptor 8 already have data ready for reading? If any of the file descriptors match the interest then <code>epoll_wait</code> can return without blocking.</p>
<p>By contrast, in edge-triggered mode the kernel skips this check and immediately puts the process to sleep when it calls <code>epoll_wait</code>. This puts all of the responsibility on you, the programmer, to do the Right Thing and fully read and write all data for each file descriptor before waiting on this.</p>
<p>This edge-triggered mode is what makes <code>epoll</code> an <em>O(1)</em> I/O multiplexer: the <code>epoll_wait</code> call will suspend immediately, and since a list is maintained for each file descriptor ahead of time, when new data comes in the kernel immediately knows what processes must be woken up in <em>O(1)</em> time.</p>
<p>Here’s a more worked out example of the difference between edge-triggered and level-triggered modes. Suppose your read buffer is 100 bytes, and 200 bytes of data comes in for that file descriptor. Suppose then you call <code>read</code> exactly one time and then call <code>epoll_wait</code> again. There’s still 100 bytes of data already ready to read. In level-triggered mode the kernel would notice this and notify the process that it should call <code>read</code> again. By contrast, in edge-triggered mode the kernel would immediately go to sleep. If the other side is expecting a response (e.g. the data it sent is some kind of RPC) then the two sides will “deadlock”, as the server will be waiting for the client to send more data, but the client will be waiting for the server to send a response.</p>
<p>To use edge-triggered polling you <strong>must</strong> put the file descriptors into nonblocking mode. Then you <strong>must</strong> call <code>read</code> or <code>write</code> until they return <code>EWOULDBLOCK</code> every time. If you fail to meet these conditions you will miss notifications from the kernel. But there’s a big upside of doing this: each call to <code>epoll_wait</code> will be more efficient, which can be very important on programs with extremely high levels of concurrency. If you want to learn more about the details I strongly encourage you to read the <a href="http://man7.org/linux/man-pages/man7/epoll.7.html">epoll(7) man page</a>.</p>
<br />
                  
              </p>
            </div>
            <div class="post_footer">
              
              
                <div class="next-post" style="margin-top: 20px;">
                  <div class="next">下一篇</div>
                  <a href="https://fforkboat.github.io/post/io-yu-huan-chong-qu/">
                    <h3 class="post-title">
                      IO与缓冲区
                    </h3>
                  </a>
                </div>
                
            </div>
          </div>
          
        </div>
      </div>
    </div>
  </div>
  <div class="footer">
  
  <div class="powered_by">
    <a href="https://github.com/tangkaichuan/gridea-theme-one" target="_blank">Theme One,</a>
    <a href="https://gridea.dev/" target="_blank">Powered by Gridea&#65281;</a>
  </div>
  <div class="footer_slogan">
    福柯船长的船坞
</div>
<div id="back_to_top" class="back_to_top">
  <span>△</span>
</div>
<script src="https://fforkboat.github.io/media/scripts/util.js"></script>

</div>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/styles/default.min.css">
  <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release/build/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
    $(function() {
        $("#toc").tocify({ selectors: "h2, h3, h4" });
    });
  </script>
</body>

</html>